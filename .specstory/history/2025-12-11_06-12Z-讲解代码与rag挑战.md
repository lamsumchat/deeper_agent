<!-- Generated by SpecStory, Markdown v2.1.0 -->

<!-- cursor Session 73de0c64-241c-45d8-b9ac-ef0a536b6800 (2025-12-11 06:12Z) -->

# 讲解代码与RAG挑战 (2025-12-11 06:12Z)

_**User (2025-12-11 06:12Z)**_

给我讲解一下这里面的关键代码，并顺便介绍业界里做RAG可能遇到的挑战以及通常是如何考虑的

---

_**Agent (model gpt-5.1-codex-max, mode Ask)**_

<think><details><summary>Thought Process</summary>
**Reading entire notebook for analysis**

I'm reading the full rag_demo.ipynb since it's manageable at 540 lines to identify important code cells for explanation.</details></think>

---

<tool-use data-tool-type="read" data-tool-name="read_file">
<details><summary>Tool use: **read_file** • Read file: /Users/lamsumchat/Documents/coding_projects/playground/ai_tutorials/agentalpha/rag-phase1/rag_demo.ipynb</summary>

</details>
</tool-use>

---

<think><details><summary>Thought Process</summary>
**Planning code citation strategy**

The challenge is referencing key functions in the Jupyter notebook with accurate line numbers for code citations; since the notebook embeds JSON, I'll use ripgrep searches to find function definitions and their line numbers for precise citations.</details></think>

---

<tool-use data-tool-type="unknown" data-tool-name="rg">
<details>
<summary>Tool use: **rg**</summary>


Parameters:

```json
{
  "pattern": "configure_settings",
  "path": "/Users/lamsumchat/Documents/coding_projects/playground/ai_tutorials/agentalpha/rag-phase1/rag_demo.ipynb",
  "outputMode": "content",
  "caseInsensitive": false
}
```
Result:

```json
{
  "success": {
    "pattern": "configure_settings",
    "path": "/Users/lamsumchat/Documents/coding_projects/playground/ai_tutorials/agentalpha/rag-phase1/rag_demo.ipynb",
    "outputMode": "content",
    "workspaceResults": {
      "/Users/lamsumchat/Documents/coding_projects/playground/ai_tutorials/agentalpha": {
        "content": {
          "matches": [
            {
              "file": "rag-phase1/rag_demo.ipynb",
              "matches": [
                {
                  "lineNumber": 120,
                  "content": "        \"def configure_settings():\\n\","
                },
                {
                  "lineNumber": 281,
                  "content": "        \"configure_settings()\\n\","
                }
              ]
            }
          ],
          "totalLines": 2,
          "totalMatchedLines": 2
        }
      }
    }
  }
}
```
Status: **completed**


</details>
</tool-use>

---

<think><details><summary>Thought Process</summary>
**Considering line-numbered snippets**

I'm exploring how to get exact line ranges with numbers for functions using ripgrep's context and numbering options to align code excerpts with their real source lines for accurate snippet citations.</details></think>

---

<tool-use data-tool-type="unknown" data-tool-name="rg">
<details>
<summary>Tool use: **rg**</summary>


Parameters:

```json
{
  "pattern": "def configure_settings",
  "path": "/Users/lamsumchat/Documents/coding_projects/playground/ai_tutorials/agentalpha/rag-phase1/rag_demo.ipynb",
  "outputMode": "content",
  "context": 8,
  "caseInsensitive": false
}
```
Result:

```json
{
  "success": {
    "pattern": "def configure_settings",
    "path": "/Users/lamsumchat/Documents/coding_projects/playground/ai_tutorials/agentalpha/rag-phase1/rag_demo.ipynb",
    "outputMode": "content",
    "workspaceResults": {
      "/Users/lamsumchat/Documents/coding_projects/playground/ai_tutorials/agentalpha": {
        "content": {
          "matches": [
            {
              "file": "rag-phase1/rag_demo.ipynb",
              "matches": [
                {
                  "lineNumber": 112,
                  "content": "      ]",
                  "isContextLine": true
                },
                {
                  "lineNumber": 113,
                  "content": "    },",
                  "isContextLine": true
                },
                {
                  "lineNumber": 114,
                  "content": "    {",
                  "isContextLine": true
                },
                {
                  "lineNumber": 115,
                  "content": "      \"cell_type\": \"code\",",
                  "isContextLine": true
                },
                {
                  "lineNumber": 116,
                  "content": "      \"execution_count\": 3,",
                  "isContextLine": true
                },
                {
                  "lineNumber": 117,
                  "content": "      \"metadata\": {},",
                  "isContextLine": true
                },
                {
                  "lineNumber": 118,
                  "content": "      \"outputs\": [],",
                  "isContextLine": true
                },
                {
                  "lineNumber": 119,
                  "content": "      \"source\": [",
                  "isContextLine": true
                },
                {
                  "lineNumber": 120,
                  "content": "        \"def configure_settings():\\n\","
                },
                {
                  "lineNumber": 121,
                  "content": "        \"    Settings.chunk_size = chunk_size\\n\",",
                  "isContextLine": true
                },
                {
                  "lineNumber": 122,
                  "content": "        \"    Settings.chunk_overlap = chunk_overlap\\n\",",
                  "isContextLine": true
                },
                {
                  "lineNumber": 123,
                  "content": "        \"    Settings.embed_model = HuggingFaceEmbedding(model_name=\\\"BAAI/bge-small-en-v1.5\\\")\\n\",",
                  "isContextLine": true
                },
                {
                  "lineNumber": 124,
                  "content": "        \"    Settings.llm = OpenRouter(model=llm_model, api_key=api_key)\\n\",",
                  "isContextLine": true
                },
                {
                  "lineNumber": 125,
                  "content": "        \"\\n\",",
                  "isContextLine": true
                },
                {
                  "lineNumber": 126,
                  "content": "        \"\\n\",",
                  "isContextLine": true
                },
                {
                  "lineNumber": 127,
                  "content": "        \"def load_web_documents(urls: List[str]) -&gt; List[Document]:\\n\",",
                  "isContextLine": true
                },
                {
                  "lineNumber": 128,
                  "content": "        \"    docs = SimpleWebPageReader(html_to_text=True).load_data(urls)\\n\",",
                  "isContextLine": true
                }
              ]
            }
          ],
          "totalLines": 17,
          "totalMatchedLines": 1
        }
      }
    }
  }
}
```
Status: **completed**


</details>
</tool-use>

---

<think><details><summary>Thought Process</summary>
**Planning multiline function extraction**

I'm preparing to gather start and end line ranges for key functions using a single ripgrep command matching their defs with context; this will help accurately cite multiple functions with their line spans.</details></think>

---

<tool-use data-tool-type="unknown" data-tool-name="rg">
<details>
<summary>Tool use: **rg**</summary>


Parameters:

```json
{
  "pattern": "def (configure_settings|load_web_documents|load_pdf_documents|load_all_documents|init_vector_store|build_or_load_index|append_citations|run_query_with_citation|demonstrate_metadata_filter|create_hybrid_retriever|create_reranker|compare_retrieval_strategies)",
  "path": "/Users/lamsumchat/Documents/coding_projects/playground/ai_tutorials/agentalpha/rag-phase1/rag_demo.ipynb",
  "outputMode": "content",
  "caseInsensitive": false
}
```
Result:

```json
{
  "success": {
    "pattern": "def (configure_settings|load_web_documents|load_pdf_documents|load_all_documents|init_vector_store|build_or_load_index|append_citations|run_query_with_citation|demonstrate_metadata_filter|create_hybrid_retriever|create_reranker|compare_retrieval_strategies)",
    "path": "/Users/lamsumchat/Documents/coding_projects/playground/ai_tutorials/agentalpha/rag-phase1/rag_demo.ipynb",
    "outputMode": "content",
    "workspaceResults": {
      "/Users/lamsumchat/Documents/coding_projects/playground/ai_tutorials/agentalpha": {
        "content": {
          "matches": [
            {
              "file": "rag-phase1/rag_demo.ipynb",
              "matches": [
                {
                  "lineNumber": 120,
                  "content": "        \"def configure_settings():\\n\","
                },
                {
                  "lineNumber": 127,
                  "content": "        \"def load_web_documents(urls: List[str]) -&gt; List[Document]:\\n\","
                },
                {
                  "lineNumber": 134,
                  "content": "        \"def load_pdf_documents(data_dir: Path) -&gt; List[Document]:\\n\","
                },
                {
                  "lineNumber": 146,
                  "content": "        \"def load_all_documents(include_pdf: bool = True) -&gt; List[Document]:\\n\","
                },
                {
                  "lineNumber": 154,
                  "content": "        \"def init_vector_store(force_rebuild: bool = False) -&gt; Tuple[ChromaVectorStore, chromadb.api.models.Collection.Collection]:\\n\","
                },
                {
                  "lineNumber": 169,
                  "content": "        \"def build_or_load_index(documents: List[Document], force_rebuild: bool = False) -&gt; VectorStoreIndex:\\n\","
                },
                {
                  "lineNumber": 181,
                  "content": "        \"def append_citations(text: str, sources: Iterable[str]) -&gt; str:\\n\","
                },
                {
                  "lineNumber": 191,
                  "content": "        \"def run_query_with_citation(query_engine, question: str, label: str) -&gt; None:\\n\","
                },
                {
                  "lineNumber": 209,
                  "content": "        \"def demonstrate_metadata_filter(index: VectorStoreIndex):\\n\","
                },
                {
                  "lineNumber": 222,
                  "content": "        \"def create_hybrid_retriever(index: VectorStoreIndex) -&gt; QueryFusionRetriever:\\n\","
                },
                {
                  "lineNumber": 236,
                  "content": "        \"def create_reranker() -&gt; SentenceTransformerRerank:\\n\","
                },
                {
                  "lineNumber": 240,
                  "content": "        \"def compare_retrieval_strategies(index: VectorStoreIndex, question: str):\\n\","
                }
              ]
            }
          ],
          "totalLines": 12,
          "totalMatchedLines": 12
        }
      }
    }
  }
}
```
Status: **completed**


</details>
</tool-use>

---

<think><details><summary>Thought Process</summary>
**Planning code citation and content extraction**

I'm preparing to extract accurate code snippets for three key functions with exact start-end line numbers from the notebook, ensuring code blocks include actual content and file path. Then I'll draft a structured Chinese explanation with bullet points covering main code parts and RAG industry challenges, citing the code with precise line references.</details></think>

---

关键代码梳理（基于 `rag-phase1/rag_demo.ipynb`）：
- 配置与默认模型：`configure_settings` 把全局的 chunk 配置、Embedding（`BAAI/bge-small-en-v1.5`）和 LLM（OpenRouter 上的 `llm_model`）写入 LlamaIndex 的 `Settings`，确保后续索引和查询统一用同一套模型/参数。  
```120:125:rag-phase1/rag_demo.ipynb
def configure_settings():
    Settings.chunk_size = chunk_size
    Settings.chunk_overlap = chunk_overlap
    Settings.embed_model = HuggingFaceEmbedding(model_name="BAAI/bge-small-en-v1.5")
    Settings.llm = OpenRouter(model=llm_model, api_key=api_key)
```
- 数据加载与索引构建：`load_all_documents` 同时支持网页 (`RAG_URLS`) 与本地 PDF；`init_vector_store`/`build_or_load_index` 用 Chroma 持久化，支持 `force_rebuild` 清空后重建；无内容时报错防止空索引。  
```146:178:rag-phase1/rag_demo.ipynb
def load_all_documents(include_pdf: bool = True) -> List[Document]:
    docs: List[Document] = []
    docs.extend(load_web_documents(RAG_URLS))
    if include_pdf:
        docs.extend(load_pdf_documents(DATA_DIR))
    return docs
...
def build_or_load_index(documents: List[Document], force_rebuild: bool = False) -> VectorStoreIndex:
    vector_store, collection = init_vector_store(force_rebuild=force_rebuild)
    if not force_rebuild and collection.count() > 0:
        return VectorStoreIndex.from_vector_store(vector_store)
    if not documents:
        raise RuntimeError("No documents to index. Provide at least one source.")
    storage_context = StorageContext.from_defaults(vector_store=vector_store)
    return VectorStoreIndex.from_documents(
        documents, storage_context=storage_context, show_progress=True
    )
```
- 查询与对比：`compare_retrieval_strategies` 依次演示三种策略：① baseline 向量检索 (`top_k=4`)，② 提高召回 (`top_k=8`)，③ 向量检索后用 BGE reranker 二次排序；`run_query_with_citation` 把结果加上来源引用，展示得分与出处。  
```240:271:rag-phase1/rag_demo.ipynb
def compare_retrieval_strategies(index: VectorStoreIndex, question: str):
    print("\n" + "=" * 80)
    print("COMPARISON: Different Retrieval Strategies")
    print("=" * 80)

    print("\n[1/3] Baseline: Vector retrieval (top_k=4)...")
    vector_engine = index.as_query_engine(similarity_top_k=vector_top_k_baseline)
    run_query_with_citation(vector_engine, question, "Strategy 1: Baseline Vector")

    print("\n[2/3] Higher recall: Vector retrieval (top_k=8)...")
    vector_engine_high = index.as_query_engine(similarity_top_k=vector_top_k_high)
    run_query_with_citation(vector_engine_high, question, "Strategy 2: Higher Recall")

    print("\n[3/3] Advanced: Vector + BGE Reranker...")
    reranker = create_reranker()
    vector_retriever = index.as_retriever(similarity_top_k=vector_top_k_rerank)
    rerank_engine = RetrieverQueryEngine.from_args(
        vector_retriever, node_postprocessors=[reranker]
    )
    run_query_with_citation(rerank_engine, question, "Strategy 3: Vector + Reranker")
```
- 其他实用函数：`demonstrate_metadata_filter` 展示按 `source_url` 过滤的检索；`create_hybrid_retriever` 结合向量 + BM25；`append_citations`/`run_query_with_citation` 统一做防幻觉提示与来源附注。

业界做 RAG 常见挑战与思路：
- 数据与切分：文档格式混杂、表格/代码/长段落容易丢结构；通常用分层切分（标题/段落/句子）、模式化处理代码或表格，并对块内加局部摘要或结构化字段。
- 检索质量：单一向量检索在长问句或多意图时易漏；常用多路检索（dense + BM25 + hybrid）、query rewrite/expansion、子问题分解，必要时 reranker（如 BGE/ColBERT）提升精排。
- 相关性与幻觉：模型可能“编故事”；策略包括在 prompt 中强制“只用上下文/否则回答无匹配”、返回引用、对答案做 groundedness 评估或 LLM self-check，必要时用规则或阈值拒答。
- 新鲜度与版本：增量数据/权限变动导致旧嵌入过期；需要定期/事件驱动重建、TTL 或版本字段、变更订阅（CDC）去更新索引。
- 成本与延迟：Embedding、rerank、长上下文会拖慢；常见手段是缓存（query/result/embedding）、分层检索（先粗选小 top_k 再精排）、模型分级（小模型召回，大模型生成），以及并发/批量化调用。
- 元数据与权限：企业里强访问控制；需在索引时写入租户/ACL、检索时做 metadata filter，避免越权召回。
- 评估与观测：离线指标（Recall@k、MRR、nDCG、覆盖率）、人工标注集、A/B；上线后需日志、Trace、提示/检索统计，便于迭代。
- 任务适配：问答、代码生成、决策不同需求；需要为任务定制 chunk 策略、检索深度、答案模板和工具调用（例如函数调用、SQL/RAG-Fusion）。

如果想进一步实验：调节 `chunk_size/chunk_overlap`、切换 `reranker_model` 或在 `create_hybrid_retriever` 中开启融合检索，观察 `compare_retrieval_strategies` 输出的答案差异与来源得分即可。

---

_**User (2025-12-11 06:12Z)**_

给关键代码加注释，规范一点

---

_**Agent (model gpt-5.1-codex-max, mode Agent)**_

<think><details><summary>Thought Process</summary>
**Planning targeted notebook comments**

I'm preparing to add clear, concise comments to key code cells in the notebook, starting with adding inline comments for environment loading, directory paths, and API key checks in the imports/config cell. I'll use precise existing cell content to generate exact diffs for the edits, applying them sequentially via the notebook editor to ensure accuracy and safety with JSON formatting.</details></think>

---

<tool-use data-tool-type="write" data-tool-name="edit_notebook">

<details><summary>Tool use: **edit_notebook** • Edit file: /Users/lamsumchat/Documents/coding_projects/playground/ai_tutorials/agentalpha/rag-phase1/rag_demo.ipynb</summary>

**Chunk 1**
Lines added: 1, lines removed: undefined

```diff
@@ -22,4 +22,5 @@
  from llama_index.vector_stores.chroma import ChromaVectorStore
  
+ # Load .env for API keys and other secrets
  load_dotenv()
  
```

**Chunk 2**
Lines added: 1, lines removed: undefined

```diff
@@ -31,4 +32,5 @@
  
  api_key = os.getenv("OPENROUTER_API_KEY")
+ # Fail fast so later OpenRouter calls have a valid key
  if not api_key:
      raise RuntimeError("Missing OPENROUTER_API_KEY in environment/.env")
```

</details>

</tool-use>

---

<think><details><summary>Thought Process</summary>
**Adding comments to cell2 hyperparameters**

I'm preparing to include brief explanatory comments for the hyperparameters in cell2 and planning to use the entire old string from that cell with the edit_notebook function.</details></think>

---

<tool-use data-tool-type="write" data-tool-name="edit_notebook">

<details><summary>Tool use: **edit_notebook** • Edit file: /Users/lamsumchat/Documents/coding_projects/playground/ai_tutorials/agentalpha/rag-phase1/rag_demo.ipynb</summary>

**Chunk 1**
Lines added: 4, lines removed: 2

```diff
@@ -3,11 +3,13 @@
  chunk_overlap = 20
  use_pdfs = True
- force_rebuild = True  # set True to clear &amp; rebuild Chroma
+ # Set True to drop existing Chroma data; keep False for incremental runs
+ force_rebuild = True
  
  # Retrieval params
  vector_top_k_baseline = 4
  vector_top_k_high = 8
  vector_top_k_rerank = 10
- reranker_model = "BAAI/bge-reranker-base"  # switch to bge-reranker-v2-m3 if multilingual
+ # Choose a reranker; swap to multilingual variant if needed
+ reranker_model = "BAAI/bge-reranker-base"
  
  # LLM model on OpenRouter
```

</details>

</tool-use>

---

<think><details><summary>Thought Process</summary>
**Adding targeted comments to large code cell**

I'm planning to add brief explanatory comments above key functions like configuring settings and vector store initialization, including noting the try-except's best-effort deletion. I'll also add a comment about the guarded query in the run_query function, crafting the entire updated cell content carefully to avoid JSON quoting issues.</details></think>

---

<tool-use data-tool-type="write" data-tool-name="edit_notebook">

<details><summary>Tool use: **edit_notebook** • Edit file: /Users/lamsumchat/Documents/coding_projects/playground/ai_tutorials/agentalpha/rag-phase1/rag_demo.ipynb</summary>

**Chunk 1**
Lines added: 1, lines removed: undefined

```diff
@@ -1,3 +1,4 @@
  def configure_settings():
+     # Set global defaults so all readers/retrievers share the same models
      Settings.chunk_size = chunk_size
      Settings.chunk_overlap = chunk_overlap
```

**Chunk 2**
Lines added: 1, lines removed: undefined

```diff
@@ -7,4 +8,5 @@
  
  def load_web_documents(urls: List[str]) -&gt; List[Document]:
+     # Fetch web pages and normalize source metadata for later citation
      docs = SimpleWebPageReader(html_to_text=True).load_data(urls)
      for doc in docs:
```

**Chunk 3**
Lines added: 1, lines removed: undefined

```diff
@@ -14,4 +16,5 @@
  
  def load_pdf_documents(data_dir: Path) -&gt; List[Document]:
+     # Best-effort load PDFs; skip silently if folder is absent
      if not data_dir.exists():
          return []
```

**Chunk 4**
Lines added: 1, lines removed: undefined

```diff
@@ -26,4 +29,5 @@
  
  def load_all_documents(include_pdf: bool = True) -&gt; List[Document]:
+     # Combine web and optional PDF sources into a single doc list
      docs: List[Document] = []
      docs.extend(load_web_documents(RAG_URLS))
```

**Chunk 5**
Lines added: 1, lines removed: undefined

```diff
@@ -34,4 +38,5 @@
  
  def init_vector_store(force_rebuild: bool = False) -&gt; Tuple[ChromaVectorStore, chromadb.api.models.Collection.Collection]:
+     # Prepare persistent Chroma collection; optionally drop for a clean rebuild
      PERSIST_DIR.mkdir(parents=True, exist_ok=True)
      client = chromadb.PersistentClient(path=str(PERSIST_DIR))
```

**Chunk 6**
Lines added: 1, lines removed: undefined

```diff
@@ -49,4 +54,5 @@
  
  def build_or_load_index(documents: List[Document], force_rebuild: bool = False) -&gt; VectorStoreIndex:
+     # Reuse existing collection when possible; otherwise rebuild with documents
      vector_store, collection = init_vector_store(force_rebuild=force_rebuild)
      if not force_rebuild and collection.count() &gt; 0:
```

**Chunk 7**
Lines added: 1, lines removed: undefined

```diff
@@ -61,4 +67,5 @@
  
  def append_citations(text: str, sources: Iterable[str]) -&gt; str:
+     # Deduplicate citations and append to the generated answer
      unique_sources = []
      for src in sources:
```

**Chunk 8**
Lines added: 1, lines removed: undefined

```diff
@@ -71,4 +78,5 @@
  
  def run_query_with_citation(query_engine, question: str, label: str) -&gt; None:
+     # Guardrail prompt to avoid hallucinated code; print answer plus sources
      guarded_question = (
          question
```

**Chunk 9**
Lines added: 1, lines removed: undefined

```diff
@@ -89,4 +97,5 @@
  
  def demonstrate_metadata_filter(index: VectorStoreIndex):
+     # Example: constrain retrieval to a single source URL via metadata filters
      first_url = RAG_URLS[0]
      filters = MetadataFilters(filters=[ExactMatchFilter(key="source_url", value=first_url)])
```

**Chunk 10**
Lines added: 1, lines removed: undefined

```diff
@@ -102,4 +111,5 @@
  
  def create_hybrid_retriever(index: VectorStoreIndex) -&gt; QueryFusionRetriever:
+     # Merge dense (vector) and sparse (BM25) signals for better recall
      vector_retriever = index.as_retriever(similarity_top_k=5)
      nodes = list(index.docstore.docs.values())
```

**Chunk 11**
Lines added: 1, lines removed: undefined

```diff
@@ -116,4 +126,5 @@
  
  def create_reranker() -&gt; SentenceTransformerRerank:
+     # Lightweight wrapper to pick the reranker model
      return SentenceTransformerRerank(model=reranker_model, top_n=4)
  
```

</details>

</tool-use>

---

<think><details><summary>Thought Process</summary>
**Preparing final summary**

I'm ensuring the final response concisely summarizes the changes, mentions the notebook file path, and notes tests were not run, per guidelines.</details></think>

---

已在 `rag-phase1/rag_demo.ipynb` 关键代码处补充简洁注释，说明参数用途、数据加载/索引策略、检索与防幻觉逻辑以及混合检索、reranker等位置，便于快速理解和修改。未运行测试/单元格。

---

_**User (2025-12-11 06:31Z)**_

这里面的collection, index是什么变量？metadata或者说citation是怎么被构建利用和传递的？还有讲讲三个strategy分别有什么不同，后两个strategy详细的机制是什么？是否能让它把检索到的原文都一起输出？（即llm最后收到的完整prompt）

---

_**Agent (model gpt-5.1-codex-max, mode Ask)**_

<think><details><summary>Thought Process</summary>
**Explaining variables and proposing retrieval output changes**

I clarified the roles of `collection` (from Chroma client) and `index` (VectorStoreIndex), explained metadata and citation handling from document loading and citation appending, and detailed retrieval strategies and reranking distinctions. I also outlined how the current code prints only citation info but not raw retrieved text, suggesting a simple patch to print full source node content and noting prompt visibility requires more instrumentation. I'll propose clear code snippets for printing retrieved text and metadata, plus explain locations and usage of these variables for user reference.</details></think>

---

下面按问题逐项说明。

### collection / index 是什么？
- `collection`：在 `init_vector_store` 中由 `chromadb.PersistentClient.get_or_create_collection` 创建（或复用）的 Chroma collection，对应底层向量存储。只在初始化时返回，用于 `count()` 判断是否已有数据、或在 `force_rebuild=True` 时删除重建。
- `index`：`build_or_load_index` 返回的 `VectorStoreIndex`（封装 ChromaVectorStore）。若已有存量向量且未重建，则 `VectorStoreIndex.from_vector_store(vector_store)` 直接挂接；否则用当前文档重建。之后所有检索（`as_query_engine` / `as_retriever`）、混合检索、rerank 都基于这个 `index`。

### metadata / citation 构建与传递
- 构建：在 `load_web_documents` 里把页面的 `url/source` 写入 `doc.metadata["source_url"]`；在 `load_pdf_documents` 里把 `file_path` 写入同名字段。这样每个 chunk 的 metadata 都带有 `source_url`。
- 检索时携带：LlamaIndex 把 metadata 保存在节点上，检索返回的 `response.source_nodes` 中每个 node 都带 `metadata`（包含 `source_url`）。
- 输出引用：`run_query_with_citation` 调 `append_citations`，从 `source_nodes` 取 `source_url/url` 去重后追加到答案尾部；同时逐个打印 `score` 和 `source`。LLM 的生成只看到检索出的文本（context），引用是生成后在本地拼上的，并不影响 LLM prompt。

### 三个 strategy 的区别与机制
- Strategy 1：纯向量检索，`similarity_top_k=4`，直接送入 LLM。
- Strategy 2：同样纯向量，但 `top_k=8` 提高召回（更多上下文，潜在更慢，可能混入噪声）。
- Strategy 3：两段式
  - 先向量检索 `top_k=10`（`vector_top_k_rerank`）。
  - 再用 BGE reranker (`SentenceTransformerRerank(top_n=4)`) 对候选节点重排并截到 4 条高相关内容，最后把这 4 条作为上下文送 LLM。

### 如何把检索到的原文一起输出（含给 LLM 的完整上下文）
当前代码只打印引用和得分，没有直接打印 node 内容或最终 prompt。可以在 `run_query_with_citation` 里增加调试输出：逐条打印 `node.get_content()`，或在调用前后打印 query 引擎构造的上下文。示例修改（可直接替换或局部粘贴到 `run_query_with_citation` 的末尾）：

```python
def run_query_with_citation(query_engine, question: str, label: str) -> None:
    guarded_question = (
        question
        + "\nOnly use provided context. If no code or answer is present, reply 'No code snippet found in docs' and do not make up code."
    )
    response = query_engine.query(guarded_question)
    cited = append_citations(
        response.response,
        (n.metadata.get("source_url") or n.metadata.get("url") for n in response.source_nodes),
    )
    print(f"\n=== {label} ===")
    print(cited)
    print("\nRetrieved sources:")
    for idx, node in enumerate(response.source_nodes, start=1):
        src = node.metadata.get("source_url") or node.metadata.get("url") or "unknown"
        print(f"- {idx}. score={node.score:.3f} source={src}")

    # 可选：打印 LLM 实际看到的上下文
    print("\n--- Retrieved context fed to LLM ---")
    for idx, node in enumerate(response.source_nodes, start=1):
        src = node.metadata.get("source_url") or node.metadata.get("url") or "unknown"
        content = (node.get_content() or "").strip()
        print(f"[{idx}] source={src} score={node.score:.3f}")
        print(content)
        print("-" * 40)
```

说明：
- 这样会把每个检索节点的完整文本输出，基本等同于 LLM prompt 中的 context。LlamaIndex 还会在内部加上系统/模板指令，但核心上下文就是这些 `source_nodes` 的内容。
- 若想只输出片段，可用 `content[:500] + "..."` 截断。
- 如果需要捕获最终送给 LLM 的完整 prompt（含模板），可考虑 LlamaIndex 的 callback/observability（如 `Settings.callback_manager` + `LlamaDebugHandler`）或 tracing，但对调试而言打印 `source_nodes` 内容通常足够。

---

_**User (2025-12-11 09:05Z)**_

那你帮我把它检索到的原文一起输出（完整）

---

_**Agent (model gpt-5.1-codex-max, mode Agent)**_

<think><details><summary>Thought Process</summary>
**Modifying query function to print full context**

I'm updating the notebook's query function to optionally print the entire retrieved context by appending a print block in cell 3, keeping comments concise and avoiding excess output.</details></think>

---

<tool-use data-tool-type="write" data-tool-name="edit_notebook">

<details><summary>Tool use: **edit_notebook** • Edit file: /Users/lamsumchat/Documents/coding_projects/playground/ai_tutorials/agentalpha/rag-phase1/rag_demo.ipynb</summary>

**Chunk 1**
Lines added: 9, lines removed: undefined

```diff
@@ -95,4 +95,13 @@
          print(f"- {idx}. score={node.score:.3f} source={src}")
  
+     # Print the exact retrieved text that was fed to the LLM
+     print("\n--- Retrieved context fed to LLM ---")
+     for idx, node in enumerate(response.source_nodes, start=1):
+         src = node.metadata.get("source_url") or node.metadata.get("url") or "unknown"
+         content = (node.get_content() or "").strip()
+         print(f"[{idx}] source={src} score={node.score:.3f}")
+         print(content)
+         print("-" * 40)
+ 
  
  def demonstrate_metadata_filter(index: VectorStoreIndex):
```

</details>

</tool-use>

---

已更新 `rag-phase1/rag_demo.ipynb`，在 `run_query_with_citation` 末尾直接打印“LLM 实际看到的上下文”：
- 逐条输出检索到的节点：source、score 以及完整文本内容，便于观察实际 prompt 所包含的原文。
- 其他逻辑保持不变。

未运行 notebook；如需查看效果，请重跑相关单元。

---

