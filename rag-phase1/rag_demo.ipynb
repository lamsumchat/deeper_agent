{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# RAG Demo Notebook (LlamaIndex + Chroma + OpenRouter)\n",
        "\n",
        "Use this notebook to experiment with chunking, filters, and retrieval strategies without rerunning a script.\n",
        "\n",
        "**Prereqs**\n",
        "- `pip install -r requirements-rag.txt`\n",
        "- Add `.env` with `OPENROUTER_API_KEY=sk-or-...`\n",
        "- Data sources: `./data/*.pdf` (optional) + web page `SOURCE_URL`.\n",
        "\n",
        "**Tips**\n",
        "- Toggle `force_rebuild` to rebuild the Chroma index.\n",
        "- Adjust `chunk_size`, `chunk_overlap`, `top_k`, and reranker model to compare quality/latency."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "from pathlib import Path\n",
        "from typing import Iterable, List, Tuple\n",
        "\n",
        "import chromadb\n",
        "from dotenv import load_dotenv\n",
        "from llama_index.core import (\n",
        "    Settings,\n",
        "    SimpleDirectoryReader,\n",
        "    StorageContext,\n",
        "    VectorStoreIndex,\n",
        ")\n",
        "from llama_index.core.postprocessor import SentenceTransformerRerank\n",
        "from llama_index.core.query_engine import RetrieverQueryEngine\n",
        "from llama_index.core.retrievers import QueryFusionRetriever\n",
        "from llama_index.core.schema import Document\n",
        "from llama_index.core.vector_stores import ExactMatchFilter, MetadataFilters\n",
        "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
        "from llama_index.llms.openrouter import OpenRouter\n",
        "from llama_index.readers.web import SimpleWebPageReader\n",
        "from llama_index.retrievers.bm25 import BM25Retriever\n",
        "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
        "\n",
        "# Load .env for API keys and other secrets\n",
        "load_dotenv()\n",
        "\n",
        "# In Jupyter, __file__ is not defined, so use current working directory\n",
        "BASE_DIR = Path(os.getcwd())\n",
        "DATA_DIR = BASE_DIR / \"data\"\n",
        "PERSIST_DIR = BASE_DIR / \"storage\" / \"chroma\"\n",
        "COLLECTION_NAME = \"rag_demo\"\n",
        "\n",
        "api_key = os.getenv(\"OPENROUTER_API_KEY\")\n",
        "# Fail fast so later OpenRouter calls have a valid key\n",
        "if not api_key:\n",
        "    raise RuntimeError(\"Missing OPENROUTER_API_KEY in environment/.env\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Hyperparameters you can tweak\n",
        "chunk_size = 512\n",
        "chunk_overlap = 20\n",
        "use_pdfs = True\n",
        "# Set True to drop existing Chroma data; keep False for incremental runs\n",
        "force_rebuild = True\n",
        "\n",
        "# Retrieval params\n",
        "vector_top_k_baseline = 4\n",
        "vector_top_k_high = 8\n",
        "vector_top_k_rerank = 10\n",
        "# Choose a reranker; swap to multilingual variant(bge-reranker-v2-m3) if needed\n",
        "reranker_model = \"BAAI/bge-reranker-base\"\n",
        "\n",
        "# LLM model on OpenRouter\n",
        "llm_model = \"openai/gpt-4o-mini\"\n",
        "\n",
        "# RAG doc URLs (web pages)\n",
        "RAG_URLS = [\n",
        "    \"https://developers.llamaindex.ai/python/framework/understanding/rag/\",\n",
        "    \"https://developers.llamaindex.ai/python/framework/understanding/rag/indexing/\",\n",
        "    \"https://developers.llamaindex.ai/python/framework/understanding/rag/loading/\",\n",
        "    \"https://developers.llamaindex.ai/python/framework/understanding/rag/loading/llamacloud/\",\n",
        "    \"https://developers.llamaindex.ai/python/framework/understanding/rag/loading/llamahub/\",\n",
        "    \"https://developers.llamaindex.ai/python/framework/understanding/rag/querying/\",\n",
        "    \"https://developers.llamaindex.ai/python/framework/understanding/rag/storing/\",\n",
        "    \"https://developers.llamaindex.ai/python/examples/llm/openrouter/#_top\"\n",
        "]\n",
        "\n",
        "# Default question with anti-hallucination guard\n",
        "QUESTION = (\n",
        "    \"how to call openrouter in llamaindex? please give me a code example. \"\n",
        "    \"If no code snippet is present in the provided docs, say 'No code snippet found in docs' \"\n",
        "    \"and do not fabricate code.\"\n",
        ")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def configure_settings():\n",
        "    # Set global defaults so all readers/retrievers share the same models\n",
        "    Settings.chunk_size = chunk_size\n",
        "    Settings.chunk_overlap = chunk_overlap\n",
        "    Settings.embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en-v1.5\")\n",
        "    Settings.llm = OpenRouter(model=llm_model, api_key=api_key)\n",
        "\n",
        "\n",
        "def load_web_documents(urls: List[str]) -> List[Document]:\n",
        "    # Fetch web pages and normalize source metadata for later citation\n",
        "    docs = SimpleWebPageReader(html_to_text=True).load_data(urls)\n",
        "    for doc in docs:\n",
        "        doc.metadata[\"source_url\"] = doc.metadata.get(\"url\") or doc.metadata.get(\"source\")\n",
        "    return docs\n",
        "\n",
        "\n",
        "def load_pdf_documents(data_dir: Path) -> List[Document]:\n",
        "    # Best-effort load PDFs; skip silently if folder is absent\n",
        "    if not data_dir.exists():\n",
        "        return []\n",
        "    reader = SimpleDirectoryReader(\n",
        "        input_dir=str(data_dir), recursive=True, required_exts=[\".pdf\"]\n",
        "    )\n",
        "    pdf_docs = reader.load_data()\n",
        "    for doc in pdf_docs:\n",
        "        doc.metadata[\"source_url\"] = str(doc.metadata.get(\"file_path\", \"pdf\"))\n",
        "    return pdf_docs\n",
        "\n",
        "\n",
        "def load_all_documents(include_pdf: bool = True) -> List[Document]:\n",
        "    # Combine web and optional PDF sources into a single doc list\n",
        "    docs: List[Document] = []\n",
        "    docs.extend(load_web_documents(RAG_URLS))\n",
        "    if include_pdf:\n",
        "        docs.extend(load_pdf_documents(DATA_DIR))\n",
        "    return docs\n",
        "\n",
        "\n",
        "def init_vector_store(force_rebuild: bool = False) -> Tuple[ChromaVectorStore, chromadb.api.models.Collection.Collection]:\n",
        "    # Prepare persistent Chroma collection; optionally drop for a clean rebuild\n",
        "    PERSIST_DIR.mkdir(parents=True, exist_ok=True)\n",
        "    client = chromadb.PersistentClient(path=str(PERSIST_DIR))\n",
        "    if force_rebuild:\n",
        "        try:\n",
        "            client.delete_collection(name=COLLECTION_NAME)\n",
        "        except Exception:\n",
        "            pass\n",
        "    collection = client.get_or_create_collection(\n",
        "        name=COLLECTION_NAME, metadata={\"description\": \"LlamaIndex RAG demo\"}\n",
        "    )\n",
        "    vector_store = ChromaVectorStore(chroma_collection=collection)\n",
        "    return vector_store, collection\n",
        "\n",
        "\n",
        "def build_or_load_index(documents: List[Document], force_rebuild: bool = False) -> VectorStoreIndex:\n",
        "    # Reuse existing collection when possible; otherwise rebuild with documents\n",
        "    vector_store, collection = init_vector_store(force_rebuild=force_rebuild)\n",
        "    if not force_rebuild and collection.count() > 0:\n",
        "        return VectorStoreIndex.from_vector_store(vector_store)\n",
        "    if not documents:\n",
        "        raise RuntimeError(\"No documents to index. Provide at least one source.\")\n",
        "    storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
        "    return VectorStoreIndex.from_documents(\n",
        "        documents, storage_context=storage_context, show_progress=True\n",
        "    )\n",
        "\n",
        "\n",
        "def append_citations(text: str, sources: Iterable[str]) -> str:\n",
        "    # Deduplicate citations and append to the generated answer\n",
        "    unique_sources = []\n",
        "    for src in sources:\n",
        "        if src and src not in unique_sources:\n",
        "            unique_sources.append(src)\n",
        "    if not unique_sources:\n",
        "        return text\n",
        "    return f\"{text} [Source: {', '.join(unique_sources)}]\"\n",
        "\n",
        "\n",
        "def run_query_with_citation(query_engine, question: str, label: str) -> None:\n",
        "    # Guardrail prompt to avoid hallucinated code; print answer plus sources\n",
        "    guarded_question = (\n",
        "        question\n",
        "        + \"\\nOnly use provided context. If no code or answer is present, reply 'No code snippet found in docs' and do not make up code.\"\n",
        "    )\n",
        "    response = query_engine.query(guarded_question)\n",
        "    cited = append_citations(\n",
        "        response.response,\n",
        "        (n.metadata.get(\"source_url\") or n.metadata.get(\"url\") for n in response.source_nodes),\n",
        "    )\n",
        "    print(f\"\\n=== {label} ===\")\n",
        "    print(cited)\n",
        "    print(\"\\nRetrieved sources:\")\n",
        "    for idx, node in enumerate(response.source_nodes, start=1):\n",
        "        src = node.metadata.get(\"source_url\") or node.metadata.get(\"url\") or \"unknown\"\n",
        "        print(f\"- {idx}. score={node.score:.3f} source={src}\")\n",
        "\n",
        "    # Print the exact retrieved text that was fed to the LLM\n",
        "    print(\"\\n--- Retrieved context fed to LLM ---\")\n",
        "    for idx, node in enumerate(response.source_nodes, start=1):\n",
        "        src = node.metadata.get(\"source_url\") or node.metadata.get(\"url\") or \"unknown\"\n",
        "        content = (node.get_content() or \"\").strip()\n",
        "        print(f\"[{idx}] source={src} score={node.score:.3f}\")\n",
        "        print(content)\n",
        "        print(\"-\" * 40)\n",
        "\n",
        "\n",
        "def demonstrate_metadata_filter(index: VectorStoreIndex):\n",
        "    # Example: constrain retrieval to a single source URL via metadata filters\n",
        "    first_url = RAG_URLS[0]\n",
        "    filters = MetadataFilters(filters=[ExactMatchFilter(key=\"source_url\", value=first_url)])\n",
        "    retriever = index.as_retriever(similarity_top_k=3, filters=filters)\n",
        "    nodes = retriever.retrieve(\"RAG system core value and main challenges?\")\n",
        "    print(\"\\n=== Filtered retrieval (source_url constrained) ===\")\n",
        "    for idx, node in enumerate(nodes, start=1):\n",
        "        src = node.metadata.get(\"source_url\") or node.metadata.get(\"url\") or \"unknown\"\n",
        "        snippet = (node.get_content() or \"\").replace(\"\\n\", \" \")[:160]\n",
        "        print(f\"- {idx}. source={src}, score={node.score:.3f}\")\n",
        "        print(f\"  snippet: {snippet}...\")\n",
        "\n",
        "\n",
        "def create_hybrid_retriever(index: VectorStoreIndex) -> QueryFusionRetriever:\n",
        "    # Merge dense (vector) and sparse (BM25) signals for better recall\n",
        "    vector_retriever = index.as_retriever(similarity_top_k=5)\n",
        "    nodes = list(index.docstore.docs.values())\n",
        "    valid_nodes = [n for n in nodes if n.get_content() and n.get_content().strip()]\n",
        "    bm25_retriever = BM25Retriever(nodes=valid_nodes, similarity_top_k=5)\n",
        "    fusion_retriever = QueryFusionRetriever(\n",
        "        retrievers=[vector_retriever, bm25_retriever],\n",
        "        similarity_top_k=4,\n",
        "        num_queries=1,\n",
        "        use_async=False,\n",
        "    )\n",
        "    return fusion_retriever\n",
        "\n",
        "\n",
        "def create_reranker() -> SentenceTransformerRerank:\n",
        "    # Lightweight wrapper to pick the reranker model\n",
        "    return SentenceTransformerRerank(model=reranker_model, top_n=4)\n",
        "\n",
        "\n",
        "def compare_retrieval_strategies(index: VectorStoreIndex, question: str):\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(\"COMPARISON: Different Retrieval Strategies\")\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "    print(\"\\n[1/3] Baseline: Vector retrieval (top_k=4)...\")\n",
        "    vector_engine = index.as_query_engine(similarity_top_k=vector_top_k_baseline)\n",
        "    run_query_with_citation(vector_engine, question, \"Strategy 1: Baseline Vector\")\n",
        "\n",
        "    print(\"\\n[2/3] Higher recall: Vector retrieval (top_k=8)...\")\n",
        "    vector_engine_high = index.as_query_engine(similarity_top_k=vector_top_k_high)\n",
        "    run_query_with_citation(vector_engine_high, question, \"Strategy 2: Higher Recall\")\n",
        "\n",
        "    print(\"\\n[3/3] Advanced: Vector + BGE Reranker...\")\n",
        "    reranker = create_reranker()\n",
        "    vector_retriever = index.as_retriever(similarity_top_k=vector_top_k_rerank)\n",
        "    rerank_engine = RetrieverQueryEngine.from_args(\n",
        "        vector_retriever, node_postprocessors=[reranker]\n",
        "    )\n",
        "    run_query_with_citation(rerank_engine, question, \"Strategy 3: Vector + Reranker\")\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(\"Comparison complete! Review answers and sources above.\")\n",
        "    print(\"=\" * 80)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Settings configured. chunk_size= 512 chunk_overlap= 20\n"
          ]
        }
      ],
      "source": [
        "configure_settings()\n",
        "print(\"Settings configured. chunk_size=\", chunk_size, \"chunk_overlap=\", chunk_overlap)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded 47 documents\n"
          ]
        }
      ],
      "source": [
        "docs = load_all_documents(include_pdf=use_pdfs)\n",
        "print(f\"Loaded {len(docs)} documents\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-12-11 21:05:21,281 - INFO - Anonymized telemetry enabled. See                     https://docs.trychroma.com/telemetry for more information.\n",
            "Parsing nodes: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 47/47 [00:01<00:00, 40.43it/s]\n",
            "Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 411/411 [00:20<00:00, 19.85it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Index ready. Collection size: 411\n"
          ]
        }
      ],
      "source": [
        "index = build_or_load_index(docs, force_rebuild=force_rebuild)\n",
        "print(\"Index ready. Collection size:\", index._vector_store._collection.count())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== Filtered retrieval (source_url constrained) ===\n",
            "- 1. source=https://developers.llamaindex.ai/python/framework/understanding/rag/, score=0.585\n",
            "  snippet: * Querying            * [ Querying ](/python/framework/understanding/rag/querying/)         * Storing            * [ Storing ](/python/framework/understanding/r...\n",
            "- 2. source=https://developers.llamaindex.ai/python/framework/understanding/rag/, score=0.573\n",
            "  snippet: [install LlamaIndex](/python/framework/getting_started/installation) and complete the [starter tutorial](/python/framework/getting_started/starter_example) befo...\n",
            "- 3. source=https://developers.llamaindex.ai/python/framework/understanding/rag/, score=0.551\n",
            "  snippet: * Observability          * [ Observability ](/python/framework/module_guides/observability/)         * Callbacks            * [ Callbacks ](/python/framework/mo...\n"
          ]
        }
      ],
      "source": [
        "demonstrate_metadata_filter(index)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "================================================================================\n",
            "COMPARISON: Different Retrieval Strategies\n",
            "================================================================================\n",
            "\n",
            "[1/3] Baseline: Vector retrieval (top_k=4)...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-12-11 21:05:48,235 - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== Strategy 1: Baseline Vector ===\n",
            "To call OpenRouter in LlamaIndex, you can use the following code example:\n",
            "\n",
            "```python\n",
            "from llama_index.llms.openrouter import OpenRouter\n",
            "from llama_index.core.llms import ChatMessage\n",
            "\n",
            "llm = OpenRouter(\n",
            "    api_key=\"<your-api-key>\",\n",
            "    max_tokens=256,\n",
            "    context_window=4096,\n",
            "    model=\"gryphe/mythomax-l2-13b\",\n",
            ")\n",
            "\n",
            "message = ChatMessage(role=\"user\", content=\"Tell me a joke\")\n",
            "resp = llm.chat([message])\n",
            "print(resp)\n",
            "```\n",
            "\n",
            "This snippet demonstrates how to set up the OpenRouter and make a chat call. [Source: https://developers.llamaindex.ai/python/framework/understanding/rag/loading/llamahub/, https://developers.llamaindex.ai/python/examples/llm/openrouter/#_top]\n",
            "\n",
            "Retrieved sources:\n",
            "- 1. score=0.619 source=https://developers.llamaindex.ai/python/framework/understanding/rag/loading/llamahub/\n",
            "- 2. score=0.616 source=https://developers.llamaindex.ai/python/examples/llm/openrouter/#_top\n",
            "- 3. score=0.615 source=https://developers.llamaindex.ai/python/examples/llm/openrouter/#_top\n",
            "- 4. score=0.609 source=https://developers.llamaindex.ai/python/examples/llm/openrouter/#_top\n",
            "\n",
            "--- Retrieved context fed to LLM ---\n",
            "[1] source=https://developers.llamaindex.ai/python/framework/understanding/rag/loading/llamahub/ score=0.619\n",
            "ai/) ü¶ô.\n",
            "LlamaHub contains a registry of open-source data connectors that you can\n",
            "easily plug into any LlamaIndex application (+ Agent Tools, and Llama Packs).\n",
            "\n",
            "![](/python/_astro/llamahub.OcTHsMNk_Z20IuFN.webp)\n",
            "\n",
            "## Usage Pattern\n",
            "\n",
            "Section titled ‚ÄúUsage Pattern‚Äù\n",
            "\n",
            "Get started with:\n",
            "\n",
            "    \n",
            "    \n",
            "    from llama_index.core import download_loader\n",
            "    \n",
            "    \n",
            "    \n",
            "    \n",
            "    from llama_index.readers.google import GoogleDocsReader\n",
            "    \n",
            "    \n",
            "    \n",
            "    \n",
            "    loader = GoogleDocsReader()\n",
            "    \n",
            "    documents = loader.load_data(document_ids=[...])\n",
            "\n",
            "## Built-in connector: SimpleDirectoryReader\n",
            "\n",
            "Section titled ‚ÄúBuilt-in connector: SimpleDirectoryReader‚Äù\n",
            "\n",
            "`SimpleDirectoryReader`. Can support parsing a wide range of file types\n",
            "including `.md`, `.pdf`, `.jpg`, `.png`, `.docx`, as well as audio and video\n",
            "types. It is available directly as part of LlamaIndex:\n",
            "\n",
            "    \n",
            "    \n",
            "    from llama_index.core import SimpleDirectoryReader\n",
            "    \n",
            "    \n",
            "    \n",
            "    \n",
            "    documents = SimpleDirectoryReader(\"./data\").load_data()\n",
            "\n",
            "## Available connectors\n",
            "\n",
            "Section titled ‚ÄúAvailable connectors‚Äù\n",
            "\n",
            "Browse [LlamaHub](https://llamahub.ai/) directly to see the hundreds of\n",
            "connectors available, including:\n",
            "\n",
            "  * [Notion](https://developers.notion.com/) (`NotionPageReader`)\n",
            "  * [Google Docs](https://developers.google.com/docs/api) (`GoogleDocsReader`)\n",
            "  * [Slack](https://api.slack.com/) (`SlackReader`)\n",
            "  * [Discord](https://discord.com/developers/docs/intro) (`DiscordReader`)\n",
            "  * [Apify Actors](https://llamahub.ai/l/apify-actor) (`ApifyActor`). Can crawl the web, scrape webpages, extract text content, download files including `.pdf`, `.jpg`, `.png`, `.docx`, etc.\n",
            "\n",
            "[ Previous  \n",
            "Loading from LlamaCloud\n",
            "](/python/framework/understanding/rag/loading/llamacloud/) [ Next  \n",
            "Querying ](/python/framework/understanding/rag/querying/)\n",
            "----------------------------------------\n",
            "[2] source=https://developers.llamaindex.ai/python/examples/llm/openrouter/#_top score=0.616\n",
            "app/profile/llamaindex.bsky.social)[GitHub](https://github.com/run-\n",
            "llama/llama_index/)\n",
            "\n",
            "Select theme DarkLightAuto\n",
            "\n",
            "On this page\n",
            "\n",
            "  * Overview\n",
            "  * Call chat with ChatMessage List\n",
            "    * Streaming\n",
            "  * Call complete with Prompt\n",
            "  * Model Configuration\n",
            "\n",
            "## On this page\n",
            "\n",
            "  * Overview\n",
            "  * Call chat with ChatMessage List\n",
            "    * Streaming\n",
            "  * Call complete with Prompt\n",
            "  * Model Configuration\n",
            "\n",
            "# OpenRouter\n",
            "\n",
            "OpenRouter provides a standardized API to access many LLMs at the best price\n",
            "offered. You can find out more on their [homepage](https://openrouter.ai).\n",
            "\n",
            "If you‚Äôre opening this Notebook on colab, you will probably need to install\n",
            "LlamaIndex ü¶ô.\n",
            "\n",
            "    \n",
            "    \n",
            "    %pip install llama-index-llms-openrouter\n",
            "    \n",
            "    \n",
            "    !pip install llama-index\n",
            "    \n",
            "    \n",
            "    from llama_index.llms.openrouter import OpenRouter\n",
            "    \n",
            "    from llama_index.core.llms import ChatMessage\n",
            "\n",
            "## Call `chat` with ChatMessage List\n",
            "\n",
            "Section titled ‚ÄúCall chat with ChatMessage List‚Äù\n",
            "\n",
            "You need to either set env var `OPENROUTER_API_KEY` or set api_key in the\n",
            "class constructor\n",
            "\n",
            "    \n",
            "    \n",
            "    # import os\n",
            "    \n",
            "    # os.environ['OPENROUTER_API_KEY'] = '<your-api-key>'\n",
            "    \n",
            "    \n",
            "    \n",
            "    \n",
            "    llm = OpenRouter(\n",
            "    \n",
            "        api_key=\"<your-api-key>\",\n",
            "    \n",
            "        max_tokens=256,\n",
            "    \n",
            "        context_window=4096,\n",
            "    \n",
            "        model=\"gryphe/mythomax-l2-13b\",\n",
            "    \n",
            "    )\n",
            "    \n",
            "    \n",
            "    message = ChatMessage(role=\"user\", content=\"Tell me a joke\")\n",
            "    \n",
            "    resp = llm.chat([message])\n",
            "    \n",
            "    print(resp)\n",
            "    \n",
            "    \n",
            "    assistant: Why did the tomato turn red? Because it saw the salad dressing!\n",
            "\n",
            "### Streaming\n",
            "\n",
            "Section titled ‚ÄúStreaming‚Äù\n",
            "\n",
            "    \n",
            "    \n",
            "    message = ChatMessage(role=\"user\", content=\"Tell me a story in 250 words\")\n",
            "    \n",
            "    resp = llm.stream_chat([message])\n",
            "    \n",
            "    for r in resp:\n",
            "    \n",
            "        print(r.delta, end=\"\")\n",
            "    \n",
            "    \n",
            "    Once upon a time, there was a young girl named Maria who lived in a small village surrounded by lush green forests.\n",
            "----------------------------------------\n",
            "[3] source=https://developers.llamaindex.ai/python/examples/llm/openrouter/#_top score=0.615\n",
            "[ Frequently Asked Questions (FAQ) ](/python/framework/getting_started/faq/)\n",
            "      * Starter Tools\n",
            "\n",
            "        * [ Starter Tools ](/python/framework/getting_started/starter_tools/)\n",
            "        * [ RAG CLI ](/python/framework/getting_started/starter_tools/rag_cli/)\n",
            "      * [ Async Programming in Python ](/python/framework/getting_started/async_python/)\n",
            "    * Learn\n",
            "\n",
            "      * [ Building an LLM application ](/python/framework/understanding/)\n",
            "      * [ Using LLMs ](/python/framework/understanding/using_llms/)\n",
            "      * Building agents\n",
            "\n",
            "        * [ Building an agent ](/python/framework/understanding/agent/)\n",
            "        * [ Using existing tools ](/python/framework/understanding/agent/tools/)\n",
            "        * [ Maintaining state ](/python/framework/understanding/agent/state/)\n",
            "        * [ Streaming output and events ](/python/framework/understanding/agent/streaming/)\n",
            "        * [ Human in the loop ](/python/framework/understanding/agent/human_in_the_loop/)\n",
            "        * [ Multi-agent patterns in LlamaIndex ](/python/framework/understanding/agent/multi_agent/)\n",
            "        * [ Using Structured Output ](/python/framework/understanding/agent/structured_output/)\n",
            "      * Building a RAG pipeline\n",
            "\n",
            "        * [ Introduction to RAG ](/python/framework/understanding/rag/)\n",
            "        * Indexing\n",
            "\n",
            "          * [ Indexing ](/python/framework/understanding/rag/indexing/)\n",
            "        * Loading\n",
            "\n",
            "          * [ Loading Data (Ingestion) ](/python/framework/understanding/rag/loading/)\n",
            "          * [ Loading from LlamaCloud\n",
            "----------------------------------------\n",
            "[4] source=https://developers.llamaindex.ai/python/examples/llm/openrouter/#_top score=0.609\n",
            "[ Module Guides ](/python/framework/module_guides/deploying/query_engine/modules/)\n",
            "          * [ Response Modes ](/python/framework/module_guides/deploying/query_engine/response_modes/)\n",
            "          * [ Streaming ](/python/framework/module_guides/deploying/query_engine/streaming/)\n",
            "          * [ Supporting Modules ](/python/framework/module_guides/deploying/query_engine/supporting_modules/)\n",
            "          * [ Usage Pattern ](/python/framework/module_guides/deploying/query_engine/usage_pattern/)\n",
            "      * Evaluating\n",
            "\n",
            "        * [ Evaluating ](/python/framework/module_guides/evaluating/)\n",
            "        * [ Contributing A `LabelledRagDataset` ](/python/framework/module_guides/evaluating/contributing_llamadatasets/)\n",
            "        * [ Evaluating Evaluators with `LabelledEvaluatorDataset`'s ](/python/framework/module_guides/evaluating/evaluating_evaluators_with_llamadatasets/)\n",
            "        * [ Evaluating With `LabelledRagDataset`'s ](/python/framework/module_guides/evaluating/evaluating_with_llamadatasets/)\n",
            "        * [ Modules ](/python/framework/module_guides/evaluating/modules/)\n",
            "        * [ Usage Pattern (Response Evaluation) ](/python/framework/module_guides/evaluating/usage_pattern/)\n",
            "        * [ Usage Pattern (Retrieval) ](/python/framework/module_guides/evaluating/usage_pattern_retrieval/)\n",
            "      * Indexing\n",
            "\n",
            "        * [ Indexing ](/python/framework/module_guides/indexing/)\n",
            "        * [ Document Management ](/python/framework/module_guides/indexing/document_management/)\n",
            "        * [ How Each Index Works ](/python/framework/module_guides/indexing/index_guide/)\n",
            "        * [ LlamaCloudIndex + LlamaCloudRetriever\n",
            "----------------------------------------\n",
            "\n",
            "[2/3] Higher recall: Vector retrieval (top_k=8)...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-12-11 21:05:52,607 - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== Strategy 2: Higher Recall ===\n",
            "```python\n",
            "from llama_index.llms.openrouter import OpenRouter\n",
            "from llama_index.core.llms import ChatMessage\n",
            "\n",
            "llm = OpenRouter(\n",
            "    api_key=\"<your-api-key>\",\n",
            "    max_tokens=256,\n",
            "    context_window=4096,\n",
            "    model=\"gryphe/mythomax-l2-13b\",\n",
            ")\n",
            "\n",
            "message = ChatMessage(role=\"user\", content=\"Tell me a joke\")\n",
            "resp = llm.chat([message])\n",
            "print(resp)\n",
            "``` [Source: https://developers.llamaindex.ai/python/framework/understanding/rag/loading/llamahub/, https://developers.llamaindex.ai/python/examples/llm/openrouter/#_top]\n",
            "\n",
            "Retrieved sources:\n",
            "- 1. score=0.619 source=https://developers.llamaindex.ai/python/framework/understanding/rag/loading/llamahub/\n",
            "- 2. score=0.616 source=https://developers.llamaindex.ai/python/examples/llm/openrouter/#_top\n",
            "- 3. score=0.615 source=https://developers.llamaindex.ai/python/examples/llm/openrouter/#_top\n",
            "- 4. score=0.609 source=https://developers.llamaindex.ai/python/examples/llm/openrouter/#_top\n",
            "- 5. score=0.605 source=https://developers.llamaindex.ai/python/examples/llm/openrouter/#_top\n",
            "- 6. score=0.604 source=https://developers.llamaindex.ai/python/examples/llm/openrouter/#_top\n",
            "- 7. score=0.599 source=https://developers.llamaindex.ai/python/examples/llm/openrouter/#_top\n",
            "- 8. score=0.598 source=https://developers.llamaindex.ai/python/examples/llm/openrouter/#_top\n",
            "\n",
            "--- Retrieved context fed to LLM ---\n",
            "[1] source=https://developers.llamaindex.ai/python/framework/understanding/rag/loading/llamahub/ score=0.619\n",
            "ai/) ü¶ô.\n",
            "LlamaHub contains a registry of open-source data connectors that you can\n",
            "easily plug into any LlamaIndex application (+ Agent Tools, and Llama Packs).\n",
            "\n",
            "![](/python/_astro/llamahub.OcTHsMNk_Z20IuFN.webp)\n",
            "\n",
            "## Usage Pattern\n",
            "\n",
            "Section titled ‚ÄúUsage Pattern‚Äù\n",
            "\n",
            "Get started with:\n",
            "\n",
            "    \n",
            "    \n",
            "    from llama_index.core import download_loader\n",
            "    \n",
            "    \n",
            "    \n",
            "    \n",
            "    from llama_index.readers.google import GoogleDocsReader\n",
            "    \n",
            "    \n",
            "    \n",
            "    \n",
            "    loader = GoogleDocsReader()\n",
            "    \n",
            "    documents = loader.load_data(document_ids=[...])\n",
            "\n",
            "## Built-in connector: SimpleDirectoryReader\n",
            "\n",
            "Section titled ‚ÄúBuilt-in connector: SimpleDirectoryReader‚Äù\n",
            "\n",
            "`SimpleDirectoryReader`. Can support parsing a wide range of file types\n",
            "including `.md`, `.pdf`, `.jpg`, `.png`, `.docx`, as well as audio and video\n",
            "types. It is available directly as part of LlamaIndex:\n",
            "\n",
            "    \n",
            "    \n",
            "    from llama_index.core import SimpleDirectoryReader\n",
            "    \n",
            "    \n",
            "    \n",
            "    \n",
            "    documents = SimpleDirectoryReader(\"./data\").load_data()\n",
            "\n",
            "## Available connectors\n",
            "\n",
            "Section titled ‚ÄúAvailable connectors‚Äù\n",
            "\n",
            "Browse [LlamaHub](https://llamahub.ai/) directly to see the hundreds of\n",
            "connectors available, including:\n",
            "\n",
            "  * [Notion](https://developers.notion.com/) (`NotionPageReader`)\n",
            "  * [Google Docs](https://developers.google.com/docs/api) (`GoogleDocsReader`)\n",
            "  * [Slack](https://api.slack.com/) (`SlackReader`)\n",
            "  * [Discord](https://discord.com/developers/docs/intro) (`DiscordReader`)\n",
            "  * [Apify Actors](https://llamahub.ai/l/apify-actor) (`ApifyActor`). Can crawl the web, scrape webpages, extract text content, download files including `.pdf`, `.jpg`, `.png`, `.docx`, etc.\n",
            "\n",
            "[ Previous  \n",
            "Loading from LlamaCloud\n",
            "](/python/framework/understanding/rag/loading/llamacloud/) [ Next  \n",
            "Querying ](/python/framework/understanding/rag/querying/)\n",
            "----------------------------------------\n",
            "[2] source=https://developers.llamaindex.ai/python/examples/llm/openrouter/#_top score=0.616\n",
            "app/profile/llamaindex.bsky.social)[GitHub](https://github.com/run-\n",
            "llama/llama_index/)\n",
            "\n",
            "Select theme DarkLightAuto\n",
            "\n",
            "On this page\n",
            "\n",
            "  * Overview\n",
            "  * Call chat with ChatMessage List\n",
            "    * Streaming\n",
            "  * Call complete with Prompt\n",
            "  * Model Configuration\n",
            "\n",
            "## On this page\n",
            "\n",
            "  * Overview\n",
            "  * Call chat with ChatMessage List\n",
            "    * Streaming\n",
            "  * Call complete with Prompt\n",
            "  * Model Configuration\n",
            "\n",
            "# OpenRouter\n",
            "\n",
            "OpenRouter provides a standardized API to access many LLMs at the best price\n",
            "offered. You can find out more on their [homepage](https://openrouter.ai).\n",
            "\n",
            "If you‚Äôre opening this Notebook on colab, you will probably need to install\n",
            "LlamaIndex ü¶ô.\n",
            "\n",
            "    \n",
            "    \n",
            "    %pip install llama-index-llms-openrouter\n",
            "    \n",
            "    \n",
            "    !pip install llama-index\n",
            "    \n",
            "    \n",
            "    from llama_index.llms.openrouter import OpenRouter\n",
            "    \n",
            "    from llama_index.core.llms import ChatMessage\n",
            "\n",
            "## Call `chat` with ChatMessage List\n",
            "\n",
            "Section titled ‚ÄúCall chat with ChatMessage List‚Äù\n",
            "\n",
            "You need to either set env var `OPENROUTER_API_KEY` or set api_key in the\n",
            "class constructor\n",
            "\n",
            "    \n",
            "    \n",
            "    # import os\n",
            "    \n",
            "    # os.environ['OPENROUTER_API_KEY'] = '<your-api-key>'\n",
            "    \n",
            "    \n",
            "    \n",
            "    \n",
            "    llm = OpenRouter(\n",
            "    \n",
            "        api_key=\"<your-api-key>\",\n",
            "    \n",
            "        max_tokens=256,\n",
            "    \n",
            "        context_window=4096,\n",
            "    \n",
            "        model=\"gryphe/mythomax-l2-13b\",\n",
            "    \n",
            "    )\n",
            "    \n",
            "    \n",
            "    message = ChatMessage(role=\"user\", content=\"Tell me a joke\")\n",
            "    \n",
            "    resp = llm.chat([message])\n",
            "    \n",
            "    print(resp)\n",
            "    \n",
            "    \n",
            "    assistant: Why did the tomato turn red? Because it saw the salad dressing!\n",
            "\n",
            "### Streaming\n",
            "\n",
            "Section titled ‚ÄúStreaming‚Äù\n",
            "\n",
            "    \n",
            "    \n",
            "    message = ChatMessage(role=\"user\", content=\"Tell me a story in 250 words\")\n",
            "    \n",
            "    resp = llm.stream_chat([message])\n",
            "    \n",
            "    for r in resp:\n",
            "    \n",
            "        print(r.delta, end=\"\")\n",
            "    \n",
            "    \n",
            "    Once upon a time, there was a young girl named Maria who lived in a small village surrounded by lush green forests.\n",
            "----------------------------------------\n",
            "[3] source=https://developers.llamaindex.ai/python/examples/llm/openrouter/#_top score=0.615\n",
            "[ Frequently Asked Questions (FAQ) ](/python/framework/getting_started/faq/)\n",
            "      * Starter Tools\n",
            "\n",
            "        * [ Starter Tools ](/python/framework/getting_started/starter_tools/)\n",
            "        * [ RAG CLI ](/python/framework/getting_started/starter_tools/rag_cli/)\n",
            "      * [ Async Programming in Python ](/python/framework/getting_started/async_python/)\n",
            "    * Learn\n",
            "\n",
            "      * [ Building an LLM application ](/python/framework/understanding/)\n",
            "      * [ Using LLMs ](/python/framework/understanding/using_llms/)\n",
            "      * Building agents\n",
            "\n",
            "        * [ Building an agent ](/python/framework/understanding/agent/)\n",
            "        * [ Using existing tools ](/python/framework/understanding/agent/tools/)\n",
            "        * [ Maintaining state ](/python/framework/understanding/agent/state/)\n",
            "        * [ Streaming output and events ](/python/framework/understanding/agent/streaming/)\n",
            "        * [ Human in the loop ](/python/framework/understanding/agent/human_in_the_loop/)\n",
            "        * [ Multi-agent patterns in LlamaIndex ](/python/framework/understanding/agent/multi_agent/)\n",
            "        * [ Using Structured Output ](/python/framework/understanding/agent/structured_output/)\n",
            "      * Building a RAG pipeline\n",
            "\n",
            "        * [ Introduction to RAG ](/python/framework/understanding/rag/)\n",
            "        * Indexing\n",
            "\n",
            "          * [ Indexing ](/python/framework/understanding/rag/indexing/)\n",
            "        * Loading\n",
            "\n",
            "          * [ Loading Data (Ingestion) ](/python/framework/understanding/rag/loading/)\n",
            "          * [ Loading from LlamaCloud\n",
            "----------------------------------------\n",
            "[4] source=https://developers.llamaindex.ai/python/examples/llm/openrouter/#_top score=0.609\n",
            "[ Module Guides ](/python/framework/module_guides/deploying/query_engine/modules/)\n",
            "          * [ Response Modes ](/python/framework/module_guides/deploying/query_engine/response_modes/)\n",
            "          * [ Streaming ](/python/framework/module_guides/deploying/query_engine/streaming/)\n",
            "          * [ Supporting Modules ](/python/framework/module_guides/deploying/query_engine/supporting_modules/)\n",
            "          * [ Usage Pattern ](/python/framework/module_guides/deploying/query_engine/usage_pattern/)\n",
            "      * Evaluating\n",
            "\n",
            "        * [ Evaluating ](/python/framework/module_guides/evaluating/)\n",
            "        * [ Contributing A `LabelledRagDataset` ](/python/framework/module_guides/evaluating/contributing_llamadatasets/)\n",
            "        * [ Evaluating Evaluators with `LabelledEvaluatorDataset`'s ](/python/framework/module_guides/evaluating/evaluating_evaluators_with_llamadatasets/)\n",
            "        * [ Evaluating With `LabelledRagDataset`'s ](/python/framework/module_guides/evaluating/evaluating_with_llamadatasets/)\n",
            "        * [ Modules ](/python/framework/module_guides/evaluating/modules/)\n",
            "        * [ Usage Pattern (Response Evaluation) ](/python/framework/module_guides/evaluating/usage_pattern/)\n",
            "        * [ Usage Pattern (Retrieval) ](/python/framework/module_guides/evaluating/usage_pattern_retrieval/)\n",
            "      * Indexing\n",
            "\n",
            "        * [ Indexing ](/python/framework/module_guides/indexing/)\n",
            "        * [ Document Management ](/python/framework/module_guides/indexing/document_management/)\n",
            "        * [ How Each Index Works ](/python/framework/module_guides/indexing/index_guide/)\n",
            "        * [ LlamaCloudIndex + LlamaCloudRetriever\n",
            "----------------------------------------\n",
            "[5] source=https://developers.llamaindex.ai/python/examples/llm/openrouter/#_top score=0.605\n",
            "* [ Agent Data ](/python/llamaagents/llamactl/agent-data-overview/)\n",
            "      * [ Agent Data (Python) ](/python/llamaagents/llamactl/agent-data-python/)\n",
            "      * [ Agent Data (JavaScript) ](/python/llamaagents/llamactl/agent-data-javascript/)\n",
            "    * llamactl Reference\n",
            "\n",
            "      * [ init ](/python/llamaagents/llamactl-reference/commands-init/)\n",
            "      * [ serve ](/python/llamaagents/llamactl-reference/commands-serve/)\n",
            "      * [ deployments ](/python/llamaagents/llamactl-reference/commands-deployments/)\n",
            "      * [ auth ](/python/llamaagents/llamactl-reference/commands-auth/)\n",
            "      * [ auth env ](/python/llamaagents/llamactl-reference/commands-auth-env/)\n",
            "      * [ pkg ](/python/llamaagents/llamactl-reference/commands-pkg/)\n",
            "  * [ Workflows API Reference üîó ](/python/workflows-api-reference/)\n",
            "  * LlamaIndex Framework\n",
            "\n",
            "    * [ Welcome to LlamaIndex ü¶ô ! ](/python/framework/)\n",
            "    * Getting Started\n",
            "\n",
            "      * [ High-Level Concepts ](/python/framework/getting_started/concepts/)\n",
            "      * [ Installation and Setup ](/python/framework/getting_started/installation/)\n",
            "      * [ How to read these docs ](/python/framework/getting_started/reading/)\n",
            "      * [ Starter Tutorial (Using OpenAI) ](/python/framework/getting_started/starter_example/)\n",
            "      * [ Starter Tutorial (Using Local LLMs) ](/python/framework/getting_started/starter_example_local/)\n",
            "      * [ Discover LlamaIndex Video Series ](/python/framework/getting_started/discover_llamaindex/)\n",
            "      * [ Frequently Asked Questions (FAQ) ](/python/framework/getting_started/faq/)\n",
            "----------------------------------------\n",
            "[6] source=https://developers.llamaindex.ai/python/examples/llm/openrouter/#_top score=0.604\n",
            "Supporting Modules\n",
            "\n",
            "        * [ Migrating from ServiceContext to Settings ](/python/framework/module_guides/supporting_modules/service_context_migration/)\n",
            "        * [ Configuring Settings ](/python/framework/module_guides/supporting_modules/settings/)\n",
            "        * [ Supporting Modules ](/python/framework/module_guides/supporting_modules/supporting_modules/)\n",
            "    * Open Source Community\n",
            "\n",
            "      * FAQ\n",
            "\n",
            "        * [ Frequently Asked Questions ](/python/framework/community/faq/)\n",
            "        * [ Chat Engines ](/python/framework/community/faq/chat_engines/)\n",
            "        * [ Documents and Nodes ](/python/framework/community/faq/documents_and_nodes/)\n",
            "        * [ Embeddings ](/python/framework/community/faq/embeddings/)\n",
            "        * [ Large Language Models ](/python/framework/community/faq/llms/)\n",
            "        * [ Query Engines ](/python/framework/community/faq/query_engines/)\n",
            "        * [ Vector Database ](/python/framework/community/faq/vector_database/)\n",
            "      * [ Full-Stack Projects ](/python/framework/community/full_stack_projects/)\n",
            "      * Integrations\n",
            "\n",
            "        * [ Integrations ](/python/framework/community/integrations/)\n",
            "        * [ ChatGPT Plugin Integrations ](/python/framework/community/integrations/chatgpt_plugins/)\n",
            "        * [ Unit Testing LLMs/RAG With DeepEval ](/python/framework/community/integrations/deepeval/)\n",
            "        * [ Fleet Context Embeddings - Building a Hybrid Search Engine for the Llamaindex Library ](/python/framework/community/integrations/fleet_libraries_context/)\n",
            "        * [ Using Graph Stores ](/python/framework/community/integrations/graph_stores/)\n",
            "        * [ Tracing with Graphsignal ](/python/framework/community/integrations/graphsignal/)\n",
            "        * [ Guidance\n",
            "----------------------------------------\n",
            "[7] source=https://developers.llamaindex.ai/python/examples/llm/openrouter/#_top score=0.599\n",
            "[ Streaming events ](/python/llamaagents/workflows/streaming/)\n",
            "      * [ Concurrent execution of workflows ](/python/llamaagents/workflows/concurrent_execution/)\n",
            "      * [ Human in the Loop ](/python/llamaagents/workflows/human_in_the_loop/)\n",
            "      * [ Customizing entry and exit points ](/python/llamaagents/workflows/customizing_entry_exit_points/)\n",
            "      * [ Drawing a Workflow ](/python/llamaagents/workflows/drawing/)\n",
            "      * [ Resource Objects ](/python/llamaagents/workflows/resources/)\n",
            "      * [ Retry steps execution ](/python/llamaagents/workflows/retry_steps/)\n",
            "      * [ Workflows from unbound functions ](/python/llamaagents/workflows/unbound_functions/)\n",
            "      * [ Run Your Workflow as a Server ](/python/llamaagents/workflows/deployment/)\n",
            "      * [ Writing durable workflows ](/python/llamaagents/workflows/durable_workflows/)\n",
            "      * [ Observability ](/python/llamaagents/workflows/observability/)\n",
            "    * llamactl\n",
            "\n",
            "      * [ Getting Started ](/python/llamaagents/llamactl/getting-started/)\n",
            "      * [ Click-to-Deploy from LlamaCloud ](/python/llamaagents/llamactl/click-to-deploy/)\n",
            "      * [ Serving your Workflows ](/python/llamaagents/llamactl/workflow-api/)\n",
            "      * [ Configuring a UI ](/python/llamaagents/llamactl/ui-build/)\n",
            "      * [ Workflow React Hooks ](/python/llamaagents/llamactl/ui-hooks/)\n",
            "      * [ Deployment Config Reference ](/python/llamaagents/llamactl/configuration-reference/)\n",
            "      * [ Continuous Deployment with GitHub Actions ](/python/llamaagents/llamactl/cd-with-github-actions/)\n",
            "      * [ Agent Data\n",
            "----------------------------------------\n",
            "[8] source=https://developers.llamaindex.ai/python/examples/llm/openrouter/#_top score=0.598\n",
            "Skip to content\n",
            "\n",
            "[ ![](/python/_astro/llamaindex-light.m3GU97oV.svg)\n",
            "![](/python/_astro/llamaindex-dark.B8ExLyh7.svg) LlamaIndex Python\n",
            "Documentation  ](/)\n",
            "\n",
            "Search ` `Ctrl``K` `\n",
            "\n",
            "Cancel\n",
            "\n",
            "[TypeScript](/typescript/framework/)\n",
            "\n",
            "[Twitter](https://x.com/llama_index)[LinkedIn](https://www.linkedin.com/company/llamaindex)[Bluesky](https://bsky.app/profile/llamaindex.bsky.social)[GitHub](https://github.com/run-\n",
            "llama/llama_index/)\n",
            "\n",
            "Select theme DarkLightAuto\n",
            "\n",
            "  * LlamaCloud\n",
            "\n",
            "    * [ Welcome to LlamaCloud ](/python/cloud/)\n",
            "    * Parse\n",
            "\n",
            "      * [ Overview of LlamaParse ](/python/cloud/llamaparse/)\n",
            "      * [ Getting Started ](/python/cloud/llamaparse/getting_started/)\n",
            "      * Presets and Modes\n",
            "\n",
            "        * [ Modes and Presets ](/python/cloud/llamaparse/presets_and_modes/presets/)\n",
            "        * [ Advanced Parsing Modes ](/python/cloud/llamaparse/presets_and_modes/advance_parsing_modes/)\n",
            "        * [ Auto Mode ](/python/cloud/llamaparse/presets_and_modes/auto_mode/)\n",
            "        * [ Output ](/python/cloud/llamaparse/presets_and_modes/output_modes/)\n",
            "      * Features\n",
            "\n",
            "        * [ Parsing options ](/python/cloud/llamaparse/features/parsing_options/)\n",
            "        * [ Multimodal Parsing ](/python/cloud/llamaparse/features/multimodal/)\n",
            "        * [ Python Usage ](/python/cloud/llamaparse/features/python_usage/)\n",
            "        * [ Layout Extraction ](/python/cloud/llamaparse/features/layout_extraction/)\n",
            "        * [ Metadata\n",
            "----------------------------------------\n",
            "\n",
            "[3/3] Advanced: Vector + BGE Reranker...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  1.46s/it]\n",
            "2025-12-11 21:06:05,507 - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== Strategy 3: Vector + Reranker ===\n",
            "To call OpenRouter in LlamaIndex, you can use the following code example:\n",
            "\n",
            "```python\n",
            "from llama_index.llms.openrouter import OpenRouter\n",
            "from llama_index.core.llms import ChatMessage\n",
            "\n",
            "llm = OpenRouter(\n",
            "    api_key=\"<your-api-key>\",\n",
            "    max_tokens=256,\n",
            "    context_window=4096,\n",
            "    model=\"gryphe/mythomax-l2-13b\",\n",
            ")\n",
            "\n",
            "message = ChatMessage(role=\"user\", content=\"Tell me a joke\")\n",
            "resp = llm.chat([message])\n",
            "print(resp)\n",
            "```\n",
            "\n",
            "For streaming, you can use:\n",
            "\n",
            "```python\n",
            "message = ChatMessage(role=\"user\", content=\"Tell me a story in 250 words\")\n",
            "resp = llm.stream_chat([message])\n",
            "for r in resp:\n",
            "    print(r.delta, end=\"\")\n",
            "``` [Source: https://developers.llamaindex.ai/python/examples/llm/openrouter/#_top]\n",
            "\n",
            "Retrieved sources:\n",
            "- 1. score=0.991 source=https://developers.llamaindex.ai/python/examples/llm/openrouter/#_top\n",
            "- 2. score=0.764 source=https://developers.llamaindex.ai/python/examples/llm/openrouter/#_top\n",
            "- 3. score=0.693 source=https://developers.llamaindex.ai/python/examples/llm/openrouter/#_top\n",
            "- 4. score=0.654 source=https://developers.llamaindex.ai/python/examples/llm/openrouter/#_top\n",
            "\n",
            "--- Retrieved context fed to LLM ---\n",
            "[1] source=https://developers.llamaindex.ai/python/examples/llm/openrouter/#_top score=0.991\n",
            "app/profile/llamaindex.bsky.social)[GitHub](https://github.com/run-\n",
            "llama/llama_index/)\n",
            "\n",
            "Select theme DarkLightAuto\n",
            "\n",
            "On this page\n",
            "\n",
            "  * Overview\n",
            "  * Call chat with ChatMessage List\n",
            "    * Streaming\n",
            "  * Call complete with Prompt\n",
            "  * Model Configuration\n",
            "\n",
            "## On this page\n",
            "\n",
            "  * Overview\n",
            "  * Call chat with ChatMessage List\n",
            "    * Streaming\n",
            "  * Call complete with Prompt\n",
            "  * Model Configuration\n",
            "\n",
            "# OpenRouter\n",
            "\n",
            "OpenRouter provides a standardized API to access many LLMs at the best price\n",
            "offered. You can find out more on their [homepage](https://openrouter.ai).\n",
            "\n",
            "If you‚Äôre opening this Notebook on colab, you will probably need to install\n",
            "LlamaIndex ü¶ô.\n",
            "\n",
            "    \n",
            "    \n",
            "    %pip install llama-index-llms-openrouter\n",
            "    \n",
            "    \n",
            "    !pip install llama-index\n",
            "    \n",
            "    \n",
            "    from llama_index.llms.openrouter import OpenRouter\n",
            "    \n",
            "    from llama_index.core.llms import ChatMessage\n",
            "\n",
            "## Call `chat` with ChatMessage List\n",
            "\n",
            "Section titled ‚ÄúCall chat with ChatMessage List‚Äù\n",
            "\n",
            "You need to either set env var `OPENROUTER_API_KEY` or set api_key in the\n",
            "class constructor\n",
            "\n",
            "    \n",
            "    \n",
            "    # import os\n",
            "    \n",
            "    # os.environ['OPENROUTER_API_KEY'] = '<your-api-key>'\n",
            "    \n",
            "    \n",
            "    \n",
            "    \n",
            "    llm = OpenRouter(\n",
            "    \n",
            "        api_key=\"<your-api-key>\",\n",
            "    \n",
            "        max_tokens=256,\n",
            "    \n",
            "        context_window=4096,\n",
            "    \n",
            "        model=\"gryphe/mythomax-l2-13b\",\n",
            "    \n",
            "    )\n",
            "    \n",
            "    \n",
            "    message = ChatMessage(role=\"user\", content=\"Tell me a joke\")\n",
            "    \n",
            "    resp = llm.chat([message])\n",
            "    \n",
            "    print(resp)\n",
            "    \n",
            "    \n",
            "    assistant: Why did the tomato turn red? Because it saw the salad dressing!\n",
            "\n",
            "### Streaming\n",
            "\n",
            "Section titled ‚ÄúStreaming‚Äù\n",
            "\n",
            "    \n",
            "    \n",
            "    message = ChatMessage(role=\"user\", content=\"Tell me a story in 250 words\")\n",
            "    \n",
            "    resp = llm.stream_chat([message])\n",
            "    \n",
            "    for r in resp:\n",
            "    \n",
            "        print(r.delta, end=\"\")\n",
            "    \n",
            "    \n",
            "    Once upon a time, there was a young girl named Maria who lived in a small village surrounded by lush green forests.\n",
            "----------------------------------------\n",
            "[2] source=https://developers.llamaindex.ai/python/examples/llm/openrouter/#_top score=0.764\n",
            "* [ Agent Data ](/python/llamaagents/llamactl/agent-data-overview/)\n",
            "      * [ Agent Data (Python) ](/python/llamaagents/llamactl/agent-data-python/)\n",
            "      * [ Agent Data (JavaScript) ](/python/llamaagents/llamactl/agent-data-javascript/)\n",
            "    * llamactl Reference\n",
            "\n",
            "      * [ init ](/python/llamaagents/llamactl-reference/commands-init/)\n",
            "      * [ serve ](/python/llamaagents/llamactl-reference/commands-serve/)\n",
            "      * [ deployments ](/python/llamaagents/llamactl-reference/commands-deployments/)\n",
            "      * [ auth ](/python/llamaagents/llamactl-reference/commands-auth/)\n",
            "      * [ auth env ](/python/llamaagents/llamactl-reference/commands-auth-env/)\n",
            "      * [ pkg ](/python/llamaagents/llamactl-reference/commands-pkg/)\n",
            "  * [ Workflows API Reference üîó ](/python/workflows-api-reference/)\n",
            "  * LlamaIndex Framework\n",
            "\n",
            "    * [ Welcome to LlamaIndex ü¶ô ! ](/python/framework/)\n",
            "    * Getting Started\n",
            "\n",
            "      * [ High-Level Concepts ](/python/framework/getting_started/concepts/)\n",
            "      * [ Installation and Setup ](/python/framework/getting_started/installation/)\n",
            "      * [ How to read these docs ](/python/framework/getting_started/reading/)\n",
            "      * [ Starter Tutorial (Using OpenAI) ](/python/framework/getting_started/starter_example/)\n",
            "      * [ Starter Tutorial (Using Local LLMs) ](/python/framework/getting_started/starter_example_local/)\n",
            "      * [ Discover LlamaIndex Video Series ](/python/framework/getting_started/discover_llamaindex/)\n",
            "      * [ Frequently Asked Questions (FAQ) ](/python/framework/getting_started/faq/)\n",
            "----------------------------------------\n",
            "[3] source=https://developers.llamaindex.ai/python/examples/llm/openrouter/#_top score=0.693\n",
            "Skip to content\n",
            "\n",
            "[ ![](/python/_astro/llamaindex-light.m3GU97oV.svg)\n",
            "![](/python/_astro/llamaindex-dark.B8ExLyh7.svg) LlamaIndex Python\n",
            "Documentation  ](/)\n",
            "\n",
            "Search ` `Ctrl``K` `\n",
            "\n",
            "Cancel\n",
            "\n",
            "[TypeScript](/typescript/framework/)\n",
            "\n",
            "[Twitter](https://x.com/llama_index)[LinkedIn](https://www.linkedin.com/company/llamaindex)[Bluesky](https://bsky.app/profile/llamaindex.bsky.social)[GitHub](https://github.com/run-\n",
            "llama/llama_index/)\n",
            "\n",
            "Select theme DarkLightAuto\n",
            "\n",
            "  * LlamaCloud\n",
            "\n",
            "    * [ Welcome to LlamaCloud ](/python/cloud/)\n",
            "    * Parse\n",
            "\n",
            "      * [ Overview of LlamaParse ](/python/cloud/llamaparse/)\n",
            "      * [ Getting Started ](/python/cloud/llamaparse/getting_started/)\n",
            "      * Presets and Modes\n",
            "\n",
            "        * [ Modes and Presets ](/python/cloud/llamaparse/presets_and_modes/presets/)\n",
            "        * [ Advanced Parsing Modes ](/python/cloud/llamaparse/presets_and_modes/advance_parsing_modes/)\n",
            "        * [ Auto Mode ](/python/cloud/llamaparse/presets_and_modes/auto_mode/)\n",
            "        * [ Output ](/python/cloud/llamaparse/presets_and_modes/output_modes/)\n",
            "      * Features\n",
            "\n",
            "        * [ Parsing options ](/python/cloud/llamaparse/features/parsing_options/)\n",
            "        * [ Multimodal Parsing ](/python/cloud/llamaparse/features/multimodal/)\n",
            "        * [ Python Usage ](/python/cloud/llamaparse/features/python_usage/)\n",
            "        * [ Layout Extraction ](/python/cloud/llamaparse/features/layout_extraction/)\n",
            "        * [ Metadata\n",
            "----------------------------------------\n",
            "[4] source=https://developers.llamaindex.ai/python/examples/llm/openrouter/#_top score=0.654\n",
            "[ Streaming events ](/python/llamaagents/workflows/streaming/)\n",
            "      * [ Concurrent execution of workflows ](/python/llamaagents/workflows/concurrent_execution/)\n",
            "      * [ Human in the Loop ](/python/llamaagents/workflows/human_in_the_loop/)\n",
            "      * [ Customizing entry and exit points ](/python/llamaagents/workflows/customizing_entry_exit_points/)\n",
            "      * [ Drawing a Workflow ](/python/llamaagents/workflows/drawing/)\n",
            "      * [ Resource Objects ](/python/llamaagents/workflows/resources/)\n",
            "      * [ Retry steps execution ](/python/llamaagents/workflows/retry_steps/)\n",
            "      * [ Workflows from unbound functions ](/python/llamaagents/workflows/unbound_functions/)\n",
            "      * [ Run Your Workflow as a Server ](/python/llamaagents/workflows/deployment/)\n",
            "      * [ Writing durable workflows ](/python/llamaagents/workflows/durable_workflows/)\n",
            "      * [ Observability ](/python/llamaagents/workflows/observability/)\n",
            "    * llamactl\n",
            "\n",
            "      * [ Getting Started ](/python/llamaagents/llamactl/getting-started/)\n",
            "      * [ Click-to-Deploy from LlamaCloud ](/python/llamaagents/llamactl/click-to-deploy/)\n",
            "      * [ Serving your Workflows ](/python/llamaagents/llamactl/workflow-api/)\n",
            "      * [ Configuring a UI ](/python/llamaagents/llamactl/ui-build/)\n",
            "      * [ Workflow React Hooks ](/python/llamaagents/llamactl/ui-hooks/)\n",
            "      * [ Deployment Config Reference ](/python/llamaagents/llamactl/configuration-reference/)\n",
            "      * [ Continuous Deployment with GitHub Actions ](/python/llamaagents/llamactl/cd-with-github-actions/)\n",
            "      * [ Agent Data\n",
            "----------------------------------------\n",
            "\n",
            "================================================================================\n",
            "Comparison complete! Review answers and sources above.\n",
            "================================================================================\n"
          ]
        }
      ],
      "source": [
        "compare_retrieval_strategies(index, QUESTION)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
